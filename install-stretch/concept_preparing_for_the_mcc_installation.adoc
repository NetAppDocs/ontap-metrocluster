---
permalink: install-stretch/concept_preparing_for_the_mcc_installation.html
sidebar: sidebar
keywords: 
summary: ''
---
= Preparing for the MetroCluster installation
:icons: font
:imagesdir: ../media/

[.lead]
As you prepare for the MetroCluster installation, you should understand the MetroCluster hardware architecture and required components.

== Differences between the ONTAP MetroCluster configurations

[.lead]
The various MetroCluster configurations have key differences in the required components.

In all configurations, each of the two MetroCluster sites is configured as an ONTAP cluster. In a two-node MetroCluster configuration, each node is configured as a single-node cluster.

|===
| Feature| IP configurations| Fabric-attached configurations| Stretch configurations
| Four- or eight-node| Two-node| Two-node bridge-attached| Two-node direct-attached
a|
Number of controllers
a|
Four
a|
Four or eight
a|
Two
a|
Two
a|
Two
a|
Uses an FC switch storage fabric
a|
No
a|
Yes
a|
Yes
a|
No
a|
No
a|
Uses an IP switch storage fabric
a|
Yes
a|
No
a|
No
a|
No
a|
No
a|
Uses FC-to-SAS bridges
a|
No
a|
Yes
a|
Yes
a|
Yes
a|
No
a|
Uses direct-attached SAS storage
a|
Yes (local attached only)
a|
No
a|
No
a|
No
a|
Yes
a|
Supports ADP
a|
Yes (starting in ONTAP 9.4)
a|
No
a|
No
a|
No
a|
No
a|
Supports local HA
a|
Yes
a|
Yes
a|
No
a|
No
a|
No
a|
Supports ONTAP AUSO
a|
No
a|
Yes
a|
Yes
a|
Yes
a|
Yes
a|
Supports unmirrored aggregates
a|
Yes (starting in ONTAP 9.8)
a|
Yes
a|
Yes
a|
Yes
a|
Yes
a|
Supports array LUNs
a|
No
a|
Yes
a|
Yes
a|
Yes
a|
Yes
a|
Supports ONTAP Mediator
a|
Yes (starting in ONTAP 9.7)
a|
No
a|
No
a|
No
a|
No
a|
Supports MetroCluster Tiebreaker
a|
Yes (not in combination with ONTAP Mediator)
a|
Yes
a|
Yes
a|
Yes
a|
Yes
|===

== Considerations for using All SAN Array systems in MetroCluster configurations

[.lead]
Some All SAN Arrays (ASAs) are supported in MetroCluster configurations. In the MetroCluster documentation, the information for AFF models applies to the corresponding ASA system. For example, all cabling and other information for the AFF A400 system also applies to the ASA AFF A400 system.

Supported platform configurations are listed in the https://hwu.netapp.com[NetApp Hardware Universe].

== Considerations for configuring cluster peering

[.lead]
Each MetroCluster site is configured as a peer to its partner site. You should be familiar with the prerequisites and guidelines for configuring the peering relationships and when deciding whether to use shared or dedicated ports for those relationships.

*Related information*

http://docs.netapp.com/ontap-9/topic/com.netapp.doc.exp-clus-peer/home.html[Cluster and SVM peering express configuration]

=== Prerequisites for cluster peering

[.lead]
Before you set up cluster peering, you should confirm that the connectivity, port, IP address, subnet, firewall, and cluster-naming requirements are met.

==== Connectivity requirements

Every intercluster LIF on the local cluster must be able to communicate with every intercluster LIF on the remote cluster.

Although it is not required, it is typically simpler to configure the IP addresses used for intercluster LIFs in the same subnet. The IP addresses can reside in the same subnet as data LIFs, or in a different subnet. The subnet used in each cluster must meet the following requirements:

* The subnet must have enough IP addresses available to allocate to one intercluster LIF per node.
+
For example, in a six-node cluster, the subnet used for intercluster communication must have six available IP addresses.

Each node must have an intercluster LIF with an IP address on the intercluster network.

Intercluster LIFs can have an IPv4 address or an IPv6 address.

NOTE: ONTAP 9 enables you to migrate your peering networks from IPv4 to IPv6 by optionally allowing both protocols to be present simultaneously on the intercluster LIFs. In earlier releases, all intercluster relationships for an entire cluster were either IPv4 or IPv6. This meant that changing protocols was a potentially disruptive event.

==== Port requirements

You can use dedicated ports for intercluster communication, or share ports used by the data network. Ports must meet the following requirements:

* All ports that are used to communicate with a given remote cluster must be in the same IPspace.
+
You can use multiple IPspaces to peer with multiple clusters. Pair-wise full-mesh connectivity is required only within an IPspace.

* The broadcast domain that is used for intercluster communication must include at least two ports per node so that intercluster communication can fail over from one port to another port.
+
Ports added to a broadcast domain can be physical network ports, VLANs, or interface groups (ifgrps).

* All ports must be cabled.
* All ports must be in a healthy state.
* The MTU settings of the ports must be consistent.

==== Firewall requirements

Firewalls and the intercluster firewall policy must allow the following protocols:

* ICMP service
* TCP to the IP addresses of all the intercluster LIFs over the ports 10000, 11104, and 11105
* Bidirectional HTTPS between the intercluster LIFs

The default intercluster firewall policy allows access through the HTTPS protocol and from all IP addresses (0.0.0.0/0). You can modify or replace the policy if necessary.

=== Considerations when using dedicated ports

[.lead]
When determining whether using a dedicated port for intercluster replication is the correct intercluster network solution, you should consider configurations and requirements such as LAN type, available WAN bandwidth, replication interval, change rate, and number of ports.

Consider the following aspects of your network to determine whether using a dedicated port is the best intercluster network solution:

* If the amount of available WAN bandwidth is similar to that of the LAN ports and the replication interval is such that replication occurs while regular client activity exists, then you should dedicate Ethernet ports for intercluster replication to avoid contention between replication and the data protocols.
* If the network utilization generated by the data protocols (CIFS, NFS, and iSCSI) is such that the network utilization is above 50 percent, then you should dedicate ports for replication to allow for nondegraded performance if a node failover occurs.
* When physical 10 GbE or faster ports are used for data and replication, you can create VLAN ports for replication and dedicate the logical ports for intercluster replication.
+
The bandwidth of the port is shared between all VLANs and the base port.

* Consider the data change rate and replication interval and whether the amount of data that must be replicated on each interval requires enough bandwidth that it might cause contention with data protocols if sharing data ports.

=== Considerations when sharing data ports

[.lead]
When determining whether sharing a data port for intercluster replication is the correct intercluster network solution, you should consider configurations and requirements such as LAN type, available WAN bandwidth, replication interval, change rate, and number of ports.

Consider the following aspects of your network to determine whether sharing data ports is the best intercluster connectivity solution:

* For a high-speed network, such as a 40-Gigabit Ethernet (40-GbE) network, a sufficient amount of local LAN bandwidth might be available to perform replication on the same 40-GbE ports that are used for data access.
+
In many cases, the available WAN bandwidth is far less than 10 GbE LAN bandwidth.

* All nodes in the cluster might have to replicate data and share the available WAN bandwidth, making data port sharing more acceptable.
* Sharing ports for data and replication eliminates the extra port counts required to dedicate ports for replication.
* The maximum transmission unit (MTU) size of the replication network will be the same size as that used on the data network.
* Consider the data change rate and replication interval and whether the amount of data that must be replicated on each interval requires enough bandwidth that it might cause contention with data protocols if sharing data ports.
* When data ports for intercluster replication are shared, the intercluster LIFs can be migrated to any other intercluster-capable port on the same node to control the specific data port that is used for replication.

== Considerations when using unmirrored aggregates

[.lead]
If your configuration includes unmirrored aggregates, you must be aware of potential access issues after switchover operations.

=== Considerations for unmirrored aggregates when doing maintenance requiring power shutdown

If you are performing negotiated switchover for maintenance reasons requiring site-wide power shutdown, you should first manually take offline any unmirrored aggregates owned by the disaster site.

If you do not, nodes at the surviving site might go down due to multi-disk panics. This could occur if switched-over unmirrored aggregates go offline or are missing because of the loss of connectivity to storage at the disaster site due to the power shutdown or a loss of ISLs.

=== Considerations for unmirrored aggregates and hierarchical namespaces

If you are using hierarchical namespaces, you should configure the junction path so that all of the volumes in that path are either on mirrored aggregates only or on unmirrored aggregates only. Configuring a mix of unmirrored and mirrored aggregates in the junction path might prevent access to the unmirrored aggregates after the switchover operation.

=== Considerations for unmirrored aggregates and CRS metadata volume and data SVM root volumes

The configuration replication service (CRS) metadata volume and data SVM root volumes must be on a mirrored aggregate. You cannot move these volumes to unmirrored aggregate. If they are on unmirrored aggregate, negotiated switchover and switchback operations are vetoed. The metrocluster check command provides a warning if this is the case.

=== Considerations for unmirrored aggregates and SVMs

SVMs should be configured on mirrored aggregates only or on unmirrored aggregates only. Configuring a mix of unmirrored and mirrored aggregates can result in a switchover operation that exceeds 120 seconds and result in a data outage if the unmirrored aggregates do not come online.

=== Considerations for unmirrored aggregates and SAN

A LUN should not be located on an unmirrored aggregate. Configuring a LUN on an unmirrored aggregate can result in a switchover operation that exceeds 120 seconds and a data outage.

== Considerations for firewall usage at MetroCluster sites

[.lead]
If you are using a firewall at a MetroCluster site, you must ensure access for certain required ports.

The following table shows TCP/UDP port usage in an external firewall positioned between two MetroCluster sites.

|===
| Traffic type| Port/services
a|
Cluster peering
a|
11104 / TCP

11105 / TCP

a|
ONTAP System Manager
a|
443 / TCP
a|
MetroCluster IP intercluster LIFs
a|
65200 / TCP

10006 / TCP and UDP

a|
Hardware assist
a|
4444 / TCP
|===

== Preconfigured settings for new MetroCluster systems from the factory

[.lead]
New MetroCluster nodes and, if present, FC-to-SAS bridges are preconfigured and MetroCluster settings are enabled in the software. In most cases, you do not need to perform the detailed procedures provided in this guide.

=== Hardware racking and cabling

Depending on the configuration you ordered, you might need to rack the systems and complete the cabling.

link:task_configuring_the_mcc_hardware_components_2_node_stretch_sas.md#[Cabling a two-node SAS-attached stretch MetroCluster configuration]

link:task_configuring_the_mcc_hardware_components_2_node_stretch_atto.md#[Cabling a two-node bridge-attached stretch MetroCluster configuration]

=== FC-to-SAS bridge configurations

For configurations using FC-to-SAS bridges, the bridges received with the new MetroCluster configuration are preconfigured and do not require additional configuration unless you want to change the names and IP addresses.

=== Software configuration of the MetroCluster configuration

Nodes received with the new MetroCluster configuration are preconfigured with a single root aggregate. Additional configuration must be performed using the detailed procedures provided in this guide.

=== Hardware setup checklist

[.lead]
You need to know which hardware setup steps were completed at the factory and which steps you need to complete at each MetroCluster site.

|===
| Step| Completed at factory| Completed by you
a|
Mount components in one or more cabinets.
a|
Yes
a|
No
a|
Position cabinets in the desired location.
a|
No
a|
YesPosition them in the original order so that the supplied cables are long enough.

a|
Connect multiple cabinets to each other, if applicable.
a|
No
a|
YesUse the cabinet interconnect kit if it is included in the order. The kit box is labeled.

a|
Secure the cabinets to the floor, if applicable.
a|
No
a|
YesUse the universal bolt-down kit if it is included in the order. The kit box is labeled.

a|
Cable the components within the cabinet.
a|
YesCables 5 meters and longer are removed for shipping and placed in the accessories box.

a|
No
a|
Connect the cables between cabinets, if applicable.
a|
No
a|
YesCables are in the accessories box.

a|
Connect management cables to the customer's network.
a|
No
a|
YesConnect them directly or through the CN1601 management switches, if present.

IMPORTANT: To avoid address conflicts, do not connect management ports to the customer's network until after you change the default IP addresses to the customer's values.

a|
Connect console ports to the customer's terminal server, if applicable.
a|
No
a|
Yes
a|
Connect the customer's data cables to the cluster.
a|
No
a|
Yes
a|
Connect the cabinets to power and power on the components.
a|
No
a|
YesPower them on in the following order:

. PDUs
. Disk shelves and FC-to-SAS bridges, if applicable
. Nodes

a|
Verify cabling by running the Config Advisor tool.
a|
No
a|
Yes
|===
