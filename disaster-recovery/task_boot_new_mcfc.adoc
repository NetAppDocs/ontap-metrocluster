---
permalink: disaster-recovery/task_boot_new_mcfc.html
sidebar: sidebar
keywords: boot, controller, modules, fc
summary: 'After aggregate healing has been completed for both the data and root aggregates, you must boot the node or nodes at the disaster site.'
---
= Booting the new controller modules (MetroCluster FC configurations)
:icons: font
:imagesdir: ../media/

[.lead]
After aggregate healing has been completed for both the data and root aggregates, you must boot the node or nodes at the disaster site.

.About this task

This task begins with the nodes showing the LOADER prompt.

.Steps

. Display the boot menu:
+
`boot_ontap menu`

. [[step2,Step 2]]From the boot menu, select option 6, *Update flash from backup config*.

. Respond `y` to the following prompt:
+
`This will replace all flash-based configuration with the last backup to disks. Are you sure you want to continue?: y`
+
The system will boot twice, the second time to load the new configuration.
+
NOTE: If you did not clear the NVRAM contents of a used replacement controller, then you might see a panic with the following message:
`PANIC: NVRAM contents are invalid...`
If this occurs, repeat <<step2>> to boot the system to the ONTAP prompt. You then need to <<Reset-the-boot-recovery, Reset the boot recovery and rdb_corrupt bootargs>>



. Mirror the root aggregate on plex 0:
.. Assign three pool0 disks to the new controller module.
.. Mirror the root aggregate pool1 plex:
+
`aggr mirror root-aggr-name`
.. Assign unowned disks to pool0 on the local node

. If you have a four-node configuration, repeat the previous steps on the other node at the disaster site.

. Refresh the MetroCluster configuration:
.. Enter advanced privilege mode:
+
`set -privilege advanced`
.. Refresh the configuration:
+
`metrocluster configure -refresh true`
.. Return to admin privilege mode:
+
`set -privilege admin`

. Confirm that the replacement nodes at the disaster site are ready for switchback:
+
`metrocluster node show`
+
The replacement nodes should be in "`waiting for switchback recovery`" mode.  If they are in "`normal`" mode instead, you can reboot the replacement nodes.  After that boot, the nodes should be in "`waiting for switchback recovery`" mode.
+
The following example shows that the replacement nodes are ready for switchback:
+
....

cluster_B::> metrocluster node show
DR                    Configuration  DR
Grp Cluster Node      State          Mirroring Mode
--- ------- --------- -------------- --------- --------------------
1   cluster_B
            node_B_1  configured     enabled   switchover completed
            node_B_2  configured     enabled   switchover completed
    cluster_A
            node_A_1  configured     enabled   waiting for switchback recovery
            node_A_2  configured     enabled   waiting for switchback recovery
4 entries were displayed.

cluster_B::>
....

.What to do next

Proceed to link:../disaster-recovery/task_complete_recovery.html[Complete the disaster recovery process].

=== [[Reset-the-boot-recovery]]Reset the boot_recovery and rdb_corrupt bootargs
include::../_include/reset_the_boot_recovery.adoc[]


// BURT 1471046 June 28th 2022
