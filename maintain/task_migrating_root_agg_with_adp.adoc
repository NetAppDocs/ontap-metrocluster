---
permalink: maintain/task_migrating_root_agg_with_adp.html
sidebar: sidebar
keywords: metrocluster, maintain, service, migrate, root, aggregate, adp, disks, partition, advanced, disk, partitioning
summary: 'You can nondisruptively migrate an existing root aggregate that is using Advanced Disk Partitioning (ADP).'
---
= Migrate a root aggregate configured with ADP in MetroCluster IP configurations
:icons: font
:imagesdir: ../media/

[lead]
You can nondisruptively migrate an existing root aggregate that is using Advanced Disk Partitioning (ADP) if the root aggregate is running out of space or if you want to switch from using low capacity SSDs to large capacity SSDs.

.About this task 

* The local high availability (HA) pair must be enabled for storage failover. 
* This procedure is nondisruptive. 
* The configuration should be in a steady state and serving data normally, without frequent disruptions. 
* During this procedure, do not perform any hardware upgrades or any other operations that might cause disruption. 
* You can only migrate one root aggregate at a time. Do not attempt to migrate two root aggregates in parallel. 

.Steps 

. Check the health of the configuration.
 .. Check that the MetroCluster is configured and in normal mode on each cluster: `metrocluster show`
+
----
cluster_A::> metrocluster show
Cluster                   Entry Name          State
------------------------- ------------------- -----------
 Local: cluster_A         Configuration state configured
                          Mode                normal
                          AUSO Failure Domain auso-on-cluster-disaster
Remote: cluster_B         Configuration state configured
                          Mode                normal
                          AUSO Failure Domain auso-on-cluster-disaster
----

 .. Check that mirroring is enabled on each node: `metrocluster node show`
+
----
cluster_A::> metrocluster node show
DR                           Configuration  DR
Group Cluster Node           State          Mirroring Mode
----- ------- -------------- -------------- --------- --------------------
1     cluster_A
              node_A_1       configured     enabled   normal
      cluster_B
              node_B_1       configured     enabled   normal
2 entries were displayed.
----

 .. Check that the MetroCluster components are healthy: `metrocluster check run`
+
----
cluster_A::> metrocluster check run

Last Checked On: 10/1/2014 16:03:37

Component           Result
------------------- ---------
nodes               ok
lifs                ok
config-replication  ok
aggregates          ok
4 entries were displayed.

Command completed. Use the "metrocluster check show -instance" command or sub-commands in "metrocluster check" directory for detailed results.
To check if the nodes are ready to do a switchover or switchback operation, run "metrocluster switchover -simulate" or "metrocluster switchback -simulate", respectively.
----

 .. Check that there are no health alerts: `system health alert show`

 . Confirm that the local HA pair is enabled for storage failover:
 `storage failover show`
+
The output should indicate that takeover is possible for both nodes:
+
----
cluster_A::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- ---------------------------
cluster-01     cluster-02      true     Connected to cluster-02

cluster-02     cluster-01      true     Connected to cluster-01
2 entries were displayed.
----


. Zero the spare disks:
+
`run * disk zero spares`

. Identify the root aggregate size:
+
`node run local aggr status -r <root_agg_name>`
+
In the following example, the root aggregate has ten disks in "Pool0" and ten disks in "Pool1".
+
----
cluster_A::*> node run local aggr status -r <root_agg_name>
Aggregate <root_agg_name> (online, raid_dp, mirrored, fast zeroed) (block checksums)
  Plex /<root_agg_name>/plex0 (online, normal, active, pool0)
    RAID group /<root_agg_name>/plex0/rg0 (normal, block checksums, max_wdbn 5767167)
 
      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      dparity   e2a.11.0.0P3    e2a   11  0   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      parity    e10b.11.3.1P3   e10b  11  1   NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.2P3    e2a   11  2   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.3P3    e2a   11  3   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e2a.11.0.4P3    e2a   11  4   NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.5P3   e10b  11  5   NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.12P3  e10b  11  12  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.13P3  e10b  11  13  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.14P3  e10b  11  14  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.15P3  e10b  11  15  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
 
  Plex /<root_agg_name>/plex2 (online, normal, active, pool1)
    RAID group /<root_agg_name>/plex2/rg0 (normal, block checksums, max_wdbn 5767167)
 
      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      dparity   0m.i2.2L1P3     0m    22  5          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      parity    0m.i1.0L36P3    0m    22  14         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i2.2L18P3    0v    22  12         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i2.3L17P3    0v    22  2          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.0L25P3    0v    22  13         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.0L40P3    0v    22  4          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i1.1L39P3    0m    22  17         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i1.1L46P3    0m    22  15         1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0m.i2.3L13P3    0m    22  0          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      0v.i1.1L26P3    0v    22  1          1 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
 
cluster_A::*>
----

. Assign the container disks. 
+
Before you assign the disks, confirm that the recommended number of spare drives are assigned to each node. These drives are partitioned before the root aggregate is migrated. For more information, see link:https://docs.netapp.com/us-en/ontap-metrocluster/install-ip/concept_considerations_drive_assignment.html[Considerations for automatic drive assignment and ADP systems in ONTAP 9.4 and later].
+
Run the following command to assign the disks: 
+
`storage disk assign -disklist 1.11.0,1.11.1,â€¦  -owner cluster-01 -pool 0`

. Identify the root partition size.
+
The root partition size depends on the number of disks available for partition on each node. NetApp recommends that at least 12 drives per node are available for partition.
+
You can use the following table to determine the root aggregate layout:
+
[cols=2*,options="header",cols="25,75"]
|===
| Number of disks to partition
| Root aggregate layout 
| 4 disks per node | 2 data drives and 2 parity drives
| 12 disks per node | 8 data drives, 2 parity drives, and 2 spare drives
| 24 disks per node | 20 data-drives, 2 parity drives, and 2 spare drives
|===
+
To identify the root partition size, you divide the total number of 4K blocks equally between all data drives.
+
For example, if you have a root aggregate layout of 8 data drives, 2 parity drives, and 2 spare drives with a root aggregate size of 112958795 blocks, you must divide 112958795 by 8 to get the root partition size.
+
(112958795 / 8) = 14119849.375 
+
After this figure is rounded up, the root partition size is 14119850.  

. Partition each disk in the root aggregate:
+
`cluster_A*> disk partition -n 3 -i 3 -b <root_partition_size> <disk_id>` 
+
. Assign the partitions.
+
NOTE: In systems using ADP, aggregates are created using partitions in which each drive is partitioned in to P1, P2 and P3 partitions. 
+
.. Assign the P3 partition to the same node that owns the container disk:
+
`storage disk assign -disk <disk_id> -root true -pool 0 -owner cluster-01`

.. Assign the P1 partition to the system with the lower system ID number in the HA pair:
+
`storage disk assign -disk <disk_id> -data1 true -pool 0 -owner cluster-01`

.. Assign the P2 partition to the system with the higher system ID number in the HA pair:
+
`storage disk assign -disk <disk_name> -data2 true -pool 0 -owner cluster-02`
+
Repeat this step for every partitioned disk.

. Run the `storage failover show` command to confirm that takeover is possible: 
+
----
cluster_A::> storage failover show
                              Takeover
Node           Partner        Possible State Description
-------------- -------------- -------- ---------------------------
cluster-01     cluster-02      true     Connected to cluster-02

cluster-02     cluster-01      true     Connected to cluster-01
2 entries were displayed.
----

. Migrate the root aggregate. 
+
For each node, perform the migration specifying the list of disks in Pool0 and the target RAID type as parameters:
+
`system node migrate-root -node cluster-01 -disklist <pool0_disk_list> -raid-type <target_raid_type>`
+

+
For example, if the root aggregate for "cluster-01" consists of ten disks with "raid_dp", the following command migrates the root aggregate:
+
----
system node migrate-root -node cluster-01 -disklist 1.11.1.P3,1.11.2.P3,1.11.3.P3,1.11.4.P3,1.11.5.P3,1.11.6.P3,1.11.7.P3,1.11.8.P3,1.11.9.P3,1.11.10.P3 -raid-type raid_dp

Warning: This is a partially automated and guided procedure for migrating the
         root aggregate on the node "cluster-01".
         Negotiated switchover is about to start.
         Warning: This operation will create a new root aggregate and replace
         the existing root on the node "cluster-01". The existing root
         aggregate will be discarded.
Do you want to continue? {y|n}: y

Info: Started migrate-root job. Run "job show -id 51 -instance" command to
      check the progress of the job.
      Once the job is complete, mirror the root aggregate using the "storage
      aggregate mirror" command
----
+
IMPORTANT: If the number of disks is not enough, add more disks or choose a different RAID type.
+
The migration process might take several minutes to complete. During migration, the node reboots several times and you might see errors on the other nodes, you can safely ignore these errors and wait for the migration process to finish.  

. Optionally, monitor the migration progress. 
+
From the second site, run:
+
`job show -id 51 -instance`

. Re-enable RAID auto-partitioning for all MetroCluster IP nodes:
+
`storage raidlm policy modify -node <node> -policy-name auto_partition_ssds_post_init -policy-type Shared-Disk -is-enable true`

. Verify that the migration was successful:
+
`run local aggr status -r <root_agg_name>`
+
----
cluster_A::*> node run local aggr status -r <root_agg_name>
Aggregate <root_agg_name> (online, raid0, fast zeroed) (block checksums)
  Plex /<root_agg_name>/plex0 (online, normal, active, pool0)
    RAID group /<root_agg_name>/plex0/rg0 (normal, block checksums, max_wdbn 6127616)
 
      RAID Disk Device  HA  SHELF BAY CHAN Pool Type  RPM  Used (MB/blks)    Phys (MB/blks)
      --------- ------  ------------- ---- ---- ---- ----- --------------    --------------
      data      e2a.11.0.16P3   e2a   11  16  NV:A   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
      data      e10b.11.3.17P3  e10b  11  17  NV:B   0 SSD-NVM   N/A 23956/6132864     23964/6134912 (fast zeroed)
 
cluster_A::*>
----

. Verify the health of the configuration by repeating Step 1. 

// 2023 July 05, BURT 1565527
