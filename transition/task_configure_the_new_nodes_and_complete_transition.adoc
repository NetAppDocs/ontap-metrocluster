---
permalink: transition/task_configure_the_new_nodes_and_complete_transition.html
sidebar: sidebar
keywords: node, complete, transition, configure, metrocluster, ip, node, configuring, disabling, disable, data, set, up, lif, bring, svm, moving, move, system, volume
summary: With the new nodes added, you must complete the transition steps and configure the MetroCluster IP nodes.
---
= Configuring the new nodes and completing transition
:icons: font
:imagesdir: ../media/

[.lead]
With the new nodes added, you must complete the transition steps and configure the MetroCluster IP nodes.

== Configuring the MetroCluster IP nodes and disabling transition

[.lead]
You must implement the MetroCluster IP connections, refresh the MetroCluster configuration, and disable transition mode.

. Form the new nodes into a DR group by issuing the following commands from controller node_A_1-IP `metrocluster configuration-settings dr-group create -partner-cluster peer-cluster-name -local-node local-controller-name -remote-node remote-controller-name``metrocluster configuration-settings dr-group show`
. Create MetroCluster IP interfaces (node_A_1-IP, node_A_2-IP, node_B_1-IP, node_B_2-IP) -- two interfaces need to be created per controller; eight interfaces in total, using the following command: `metrocluster configuration-settings interface create -cluster-name cluster-name -home-node controller-name -home-port port -address ip-address -netmask netmask -vlan-id vlan-id``metrocluster configuration-settings interface show`
+
The -vlan-id paramter is required only if you are not using the default VLAN IDs. Only certain systems support non-default VLAnN IDs.
+
// include reference
include::_include/models_supporting_vlan-id.adoc[]
// end include reference

. Perform the MetroCluster connect operation from controller node_A_1-IP to connect the MetroCluster sites -- this operation can take a few minutes to complete. `metrocluster configuration-settings connection connect`
. Verify that the remote cluster disks are visible from each controller via the iSCSI connections: `disk show`
+
You should see the remote disks belonging to the other nodes in the configuration.

. Mirror the root aggregate for node_A_1-IP and node_B_1-IP: `aggregate mirror -aggregate root-aggr`
. Assign disks for node_A_2-IP and node_B_2-IP.
+
Pool 1 disk assignments where already made for node_A_1-IP and node_B_1-IP when the boot_after_mcc_transtion command was issued at the boot menu.

 .. Issue the following commands on node_A_2-IP: `+disk assign disk1disk2disk3 ... diskn -sysid node_B_2-IP-controller-sysid -pool 1 -force+`
 .. Issue the following commands on node_B_2-IP: `+disk assign disk1disk2disk3 ... diskn -sysid node_A_2-IP-controller-sysid -pool 1 -force+`

. Confirm ownership has been updated for the remote disks: `disk show`
. If necessary, refresh the ownership information using the following commands:
 .. Go to advanced privilege mode and enter y when prompted to continue: `set priv advanced`
 .. Refresh disk ownership: `disk refresh-ownership controller-name`
 .. Return to admin mode: `set priv admin`
. Mirror the root aggregates for node_A_2-IP and node_B_2-IP: `aggregate mirror -aggregate root-aggr`
. Verify that the aggregate re-synchronization has completed for root and data aggregates: `aggr show``aggr plex show`
+
The resync can take some time but must complete before proceeding with the following steps.

. Refresh the Metrocluster configuration to incorporate the new nodes:
 .. Go to advanced privilege mode and enter y when prompted to continue: `set priv advanced`
 .. Refresh the configuration:
+
[options="header"]
|===
| If you have configured...| Issue this command...
a|
A single aggregate in each cluster:
a|
`metrocluster configure -refresh true -allow-with-one-aggregate true`
a|
More than a single aggregate in each cluster
a|
`metrocluster configure -refresh true`
|===

 .. Return to admin mode: `set priv admin`
. Disable MetroCluster transition mode:
 .. Enter advanced privilege mode and enter y when prompted to continue: `set priv advanced`
 .. Disable transition mode: `metrocluster transition disable`
 .. Return to admin mode: `set priv admin`

== Setting up data LIFs on the new nodes

[.lead]
You must configure data LIFs on the new nodes, node_A_2-IP and node_B_2-IP.

You must add any new ports available on new controllers to a broadcast domain if not already assigned to one. If required, create VLANs or interface groups on the new ports. See the _Network Management Guide_.

https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/home.html[Network and LIF management]

. Run the following commands to identify the current port usage and broadcast domains: `network port show``network port broadcast-domain show`
. Add ports to broadcast domains and VLANs as necessary.
 .. View the IP spaces: `network ipspace show`
 .. Create IP spaces and assign data ports as needed.
+
http://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/GUID-69120CF0-F188-434F-913E-33ACB8751A5D.html[Configuring IPspaces (cluster administrators only)]

 .. View the broadcast domains: `network port broadcast-domain show`
 .. Add any data ports to a broadcast domain as needed.
+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/GUID-003BDFCD-58A3-46C9-BF0C-BA1D1D1475F9.html[Adding or removing ports from a broadcast domain]

 .. Recreate VLANs and interface groups as needed.
+
VLAN and interface group membership might be different than that of the old node.
+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/GUID-8929FCE2-5888-4051-B8C0-E27CAF3F2A63.html[Creating a VLAN]
+
https://docs.netapp.com/ontap-9/topic/com.netapp.doc.dot-cm-nmg/GUID-DBC9DEE2-EAB7-430A-A773-4E3420EE2AA1.html[Combining physical ports to create interface groups]
. Verify that the LIFs are hosted on the appropriate node and ports on the MetroCluster IP nodes (including the SVM with -mc vserver) as needed.
+
See the information gathered in xref:task_connect_the_mcc_ip_controller_modules_2n_mcc_transition_supertask.adoc[Creating the network configuration].

 .. Run the below command to check the home port of the LIFs: `network interface show -field home-port`
 .. If necessary, modify the LIF configuration: `vserver config override -command "network interface modify -vserver vserver_name -home-port active_port_after_upgrade -lif lif_name -home- node new_node_name"`
 .. Revert the LIFs to their home ports: `network interface revert * -vserver vserver_name`

== Bringing up the SVMs

[.lead]
Due to the changes if LIF configuration, you must restart the SVMs on the new nodes.

. Check the state of the SVMs: `metrocluster vserver show`
. Restart the SVMs on cluster_A that do not have an -mc suffix: `vserver start -vserver svm-name -force true`
. Repeat the previous steps on the partner cluster.
. Check that all SVMs are in a healthy state: `metrocluster vserver show`
. Verify that all data LIFs are online: `network interface show`

== Moving a system volume to the new nodes

[.lead]
To improve resiliency, a system volume should be moved from controller node_A_1-IP to controller node_A_2-IP, and also from node_B_1-IP to node_B_2-IP. You must create a mirrored aggregate on the destination node for the system volume.

System volumes have the name form MDV_CRS_*_A or MDV_CRS_*_B. _A and _B are unrelated to the site_A and site_B references used throughout this section; e.g., MDV_CRS_*_A is not associated with site_A.

. Assign at least three pool 0 and three pool 1 disks each for controllers node_A_2-IP and node_B_2-IP as needed.
. Enable disk auto-assignment.
. Move the _B system volume from node_A_1-IP to node_A_2-IP using the following steps from site_A.
 .. Create a mirrored aggregate on controller node_A_2-IP to hold the system volume: `aggr create -aggregate new_node_A_2-IP_aggr -diskcount 10 -mirror true -node nodename_node_A_2-IP``aggr show`
+
The mirrored aggregate requires five pool 0 and five pool 1 spare disks owned by controller node_A_2-IP.
+
The advanced option, "-force-small-aggregate true" can be used to limit disk use to 3 pool 0 and 3 pool 1 disks, if disks are in short supply.

 .. List the system volumes associated with the admin SVM: `vserver show``volume show -vserver admin-vserver-name`
+
You should identify volumes contained by aggregates owned by site_A. site_B system volumes will also be shown.
. Move the MDV_CRS_*_B system volume for site_A to the mirrored aggregate created on controller node_A_2-IP
 .. Check for possible destination aggregates: `volume move target-aggr show -vserver admin-vserver-name -volume system_vol_MDV_B`
+
The newly created aggregate on node_A_2-IP should be listed.

 .. Move the volume to the newly created aggregate on node_A_2-IP: `set advanced``volume move start -vserver admin-vserver -volume system_vol_MDV_B -destination-aggregate new_node_A_2-IP_aggr -cutover-window 40`
 .. Check status for the move operation: `volume move show -vserver admin-vserver-name -volume system_vol_MDV_B`
 .. When the move operation complete, verify the MDV_CRS_*_B system is contained by the new aggregate on node_A_2-IP: `set admin``volume show -vserver admin-vserver`
. Repeat the above steps on site_B (node_B_1-IP and node_B_2-IP).
