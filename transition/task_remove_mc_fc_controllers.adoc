---
permalink: transition/task_remove_mc_fc_controllers.html
sidebar: sidebar
keywords: Generating and applying RCFs to the new IP switches, review, requirement, preparing, prepare, transition, perform, procedure, order, completing, complete, step, task, moving, move, controller, storage, shelves, shelf, exist, configuration, direct, metrocluster, fc, ip, verify, health, removing, remove, tiebreaker, monitor, software, generating, generate, apply, rcf, switch, controller, configure, prepare, preparing
summary: To prepare the configuration for transition you add the new nodes to the existing MetroCluster configuration and then move data to the new nodes.
---

= Removing the MetroCluster FC controllers
:icons: font
:imagesdir: ../media/

[.lead]
You must perform clean-up tasks and remove the old controller modules from the MetroCluster configuration.

. To prevent automatic support case generation, send an Autosupport message to indicate maintenance is underway.
 .. Issue the following command: `system node autosupport invoke -node * -type all -message MAINT=maintenance-window-in-hours`
+
maintenance-window-in-hours specifies the length of the maintenance window, with a maximum of 72 hours. If the maintenance is completed before the time has elapsed, you can invoke an AutoSupport message indicating the end of the maintenance period:``system node autosupport invoke -node * -type all -message MAINT=end``

 .. Repeat the command on the partner cluster.
. Identify the aggregates hosted on the MetroCluster FC configuration that need to be deleted.
+
In this example the following data aggregates are hosted by the MetroCluster FC cluster_B and need to be deleted: aggr_data_a1 and aggr_data_a2.
+
NOTE: You need to perform the steps to identify, offline and delete the data aggregates on both the clusters. The example is for one cluster only.
+

....
cluster_B::> aggr show

Aggregate     Size Available Used% State   #Vols  Nodes            RAID Status
--------- -------- --------- ----- ------- ------ ---------------- ------------
aggr0_node_A_1-FC
           349.0GB   16.83GB   95% online       1 node_A_1-FC      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr0_node_A_2-FC
           349.0GB   16.83GB   95% online       1 node_A_2-FC      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr0_node_A_3-IP
           467.6GB   22.63GB   95% online       1 node_A_3-IP      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr0_node_A_3-IP
           467.6GB   22.62GB   95% online       1 node_A_4-IP      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr_data_a1
            1.02TB    1.02TB    0% online       0 node_A_1-FC      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr_data_a2
            1.02TB    1.02TB    0% online       0 node_A_2-FC      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr_data_a3
            1.37TB    1.35TB    1% online       3 node_A_3-IP      raid_dp,
                                                                   mirrored,
                                                                   normal
aggr_data_a4
            1.25TB    1.24TB    1% online       2 node_A_4-IP      raid_dp,
                                                                   mirrored,
                                                                   normal
8 entries were displayed.

cluster_B::>
....

. Check if the data aggregates on the FC nodes have any MDV_aud volumes, and delete them prior to deleting the aggregates.
+
You must delete the MDV_aud volumes as they cannot be moved.

. Take each of the data aggregates offline, and then delete them:
 .. Take the aggregate offline: `storage aggregate offline -aggregate aggregate-name`
+
The following example shows the aggregate aggr_data_a1 being taken offline:
+
....
cluster_B::> storage aggregate offline -aggregate aggr_data_a1

Aggregate offline successful on aggregate: aggr_data_a1
....

 .. Delete the aggregate: `storage aggregate delete -aggregate aggregate-name`
+
You can destroy the plex when prompted.
+
The following example shows the aggregate aggr_data_a1 being deleted.
+
....
cluster_B::> storage aggregate delete -aggregate aggr_data_a1
Warning: Are you sure you want to destroy aggregate "aggr_data_a1"? {y|n}: y
[Job 123] Job succeeded: DONE

cluster_B::>
....
. Identify the MetroCluster FC DR group that need to be removed.
+
In the following example the MetroCluster FC nodes are in DR Group '1', and this is the DR group that need to be removed.
+
....
cluster_B::> metrocluster node show

DR                               Configuration  DR
Group Cluster Node               State          Mirroring Mode
----- ------- ------------------ -------------- --------- --------------------
1     cluster_A
              node_A_1-FC        configured     enabled   normal
              node_A_2-FC        configured     enabled   normal
      cluster_B
              node_B_1-FC        configured     enabled   normal
              node_B_2-FC        configured     enabled   normal
2     cluster_A
              node_A_3-IP        configured     enabled   normal
              node_A_4-IP        configured     enabled   normal
      cluster_B
              node_B_3-IP        configured     enabled   normal
              node_B_3-IP        configured     enabled   normal
8 entries were displayed.

cluster_B::>
....

. Move the cluster management LIF from a MetroCluster FC node to a MetroCluster IP node: `cluster_B::> network interface migrate -vserver svm-name -lif cluster_mgmt -destination-node node-in-metrocluster-ip-dr-group -destination-port available-port`
. Change the home node and home port of the cluster management LIF: `cluster_B::> network interface modify -vserver svm-name -lif cluster_mgmt -service-policy default-management -home-node node-in-metrocluster-ip-dr-group -home-port lif-port`
. Move epsilon from a MetroCluster FC node to a MetroCluster IP node:
 .. Identify which node currently has epsilon: `cluster show -fields epsilon`
+
....
cluster_B::> cluster show -fields epsilon
node             epsilon
---------------- -------
node_A_1-FC      true
node_A_2-FC      false
node_A_1-IP      false
node_A_2-IP      false
4 entries were displayed.
....

 .. Set epsilon to false on the MetroCluster FC node (node_A_1-FC): `cluster modify -node fc-node -epsilon false`
 .. Set epsilon to true on the MetroCluster IP node (node_A_1-IP): `cluster modify -node ip-node -epsilon true`
 .. Verify that epsilon has moved to the correct node: `cluster show -fields epsilon`
+
....
cluster_B::> cluster show -fields epsilon
node             epsilon
---------------- -------
node_A_1-FC      false
node_A_2-FC      false
node_A_1-IP      true
node_A_2-IP      false
4 entries were displayed.
....
. Modify the IP address for the cluster peer of the transitioned IP nodes for each cluster:
.. Identify the cluster_A peer by using the `cluster peer show` command:
+
----
cluster_A::> cluster peer show
Peer Cluster Name         Cluster Serial Number Availability   Authentication
------------------------- --------------------- -------------- --------------
cluster_B         1-80-000011           Unavailable    absent
----

... Modify the cluster_A peer IP address:
+
`cluster peer modify -cluster cluster_A -peer-addrs node_A_3_IP -address-family ipv4`


.. Identify the cluster_B peer by using the `cluster peer show` command:
+
----
cluster_B::> cluster peer show
Peer Cluster Name         Cluster Serial Number Availability   Authentication
------------------------- --------------------- -------------- --------------
cluster_A         1-80-000011           Unavailable    absent
----
... Modify the cluster_B peer IP address:
+
`cluster peer modify -cluster cluster_B -peer-addrs node_B_3_IP -address-family ipv4`

.. Verify that the cluster peer IP address is updated for each cluster:
... Verify that the IP address is updated for each cluster by using the `cluster peer show -instance` command.
+
The `Remote Intercluster Addresses` field in the following examples displays the updated IP address. 
+
Example for cluster_A:
+
-----
cluster_A::> cluster peer show -instance

Peer Cluster Name: cluster_B
           Remote Intercluster Addresses: 172.21.178.204, 172.21.178.212 
      Availability of the Remote Cluster: Available
                     Remote Cluster Name: cluster_B
                     Active IP Addresses: 172.21.178.212, 172.21.178.204
                   Cluster Serial Number: 1-80-000011
                    Remote Cluster Nodes: node_B_3-IP,
                                          node_B_4-IP
                   Remote Cluster Health: true
                 Unreachable Local Nodes: -
          Address Family of Relationship: ipv4
    Authentication Status Administrative: use-authentication
       Authentication Status Operational: ok
                        Last Update Time: 4/20/2023 18:23:53
            IPspace for the Relationship: Default
Proposed Setting for Encryption of Inter-Cluster Communication: -
Encryption Protocol For Inter-Cluster Communication: tls-psk
  Algorithm By Which the PSK Was Derived: jpake

cluster_A::>

-----
+
Example for cluster_B
+
-----
cluster_B::> cluster peer show -instance

                       Peer Cluster Name: cluster_A
           Remote Intercluster Addresses: 172.21.178.188, 172.21.178.196 <<<<<<<< Should reflect the modified address
      Availability of the Remote Cluster: Available
                     Remote Cluster Name: cluster_A
                     Active IP Addresses: 172.21.178.196, 172.21.178.188
                   Cluster Serial Number: 1-80-000011
                    Remote Cluster Nodes: node_A_3-IP,
                                          node_A_4-IP
                   Remote Cluster Health: true
                 Unreachable Local Nodes: -
          Address Family of Relationship: ipv4
    Authentication Status Administrative: use-authentication
       Authentication Status Operational: ok
                        Last Update Time: 4/20/2023 18:23:53
            IPspace for the Relationship: Default
Proposed Setting for Encryption of Inter-Cluster Communication: -
Encryption Protocol For Inter-Cluster Communication: tls-psk
  Algorithm By Which the PSK Was Derived: jpake

cluster_B::>
-----
. On each cluster, remove the DR group containing the old nodes from the MetroCluster FC configuration.
+
You must perform this step on both clusters, one at a time.
+
....
cluster_B::> metrocluster remove-dr-group -dr-group-id 1

Warning: Nodes in the DR group that are removed from the MetroCluster
         configuration will lose their disaster recovery protection.

         Local nodes "node_A_1-FC, node_A_2-FC" will be removed from the
         MetroCluster configuration. You must repeat the operation on the
         partner cluster "cluster_B" to remove the remote nodes in the DR group.
Do you want to continue? {y|n}: y

Info: The following preparation steps must be completed on the local and partner
      clusters before removing a DR group.

      1. Move all data volumes to another DR group.
      2. Move all MDV_CRS metadata volumes to another DR group.
      3. Delete all MDV_aud metadata volumes that may exist in the DR group to
      be removed.
      4. Delete all data aggregates in the DR group to be removed. Root
      aggregates are not deleted.
      5. Migrate all data LIFs to home nodes in another DR group.
      6. Migrate the cluster management LIF to a home node in another DR group.
      Node management and inter-cluster LIFs are not migrated.
      7. Transfer epsilon to a node in another DR group.

      The command is vetoed ifthe preparation steps are not completed on the
      local and partner clusters.
Do you want to continue? {y|n}: y
[Job 513] Job succeeded: Remove DR Group is successful.

cluster_B::>
....

. Verify that the nodes are ready to be removed from the clusters.
+
You must perform this step on both clusters.
+
NOTE: At this point, the `metrocluster node show` command only shows the local MetroCluster FC nodes and no longer shows the nodes that are part of the partner cluster.
+

....
cluster_B::> metrocluster node show

DR                               Configuration  DR
Group Cluster Node               State          Mirroring Mode
----- ------- ------------------ -------------- --------- --------------------
1     cluster_A
              node_A_1-FC        ready to configure
                                                -         -
              node_A_2-FC        ready to configure
                                                -         -
2     cluster_A
              node_A_3-IP        configured     enabled   normal
              node_A_4-IP        configured     enabled   normal
      cluster_B
              node_B_3-IP        configured     enabled   normal
              node_B_4-IP        configured     enabled   normal
6 entries were displayed.

cluster_B::>
....

. Disable storage failover for the MetroCluster FC nodes.
+
You must perform this step on each node.
+
....
cluster_A::> storage failover modify -node node_A_1-FC -enabled false
cluster_A::> storage failover modify -node node_A_2-FC -enabled false
cluster_A::>
....

. Unjoin the MetroCluster FC nodes from the clusters: `cluster unjoin -node node-name`
+
You must perform this step on each node.
+
....
cluster_A::> cluster unjoin -node node_A_1-FC

Warning: This command will remove node "node_A_1-FC"from the cluster. You must
         remove the failover partner as well. After the node is removed, erase
         its configuration and initialize all disks by usingthe "Clean
         configuration and initialize all disks (4)" option from the boot menu.
Do you want to continue? {y|n}: y
[Job 553] Job is queued: Cluster remove-node of Node:node_A_1-FC with UUID:6c87de7e-ff54-11e9-8371
[Job 553] Checking prerequisites
[Job 553] Cleaning cluster database
[Job 553] Job succeeded: Node remove succeeded
If applicable, also remove the node's HA partner, and then clean its configuration and initialize all disks with the boot menu.
Run "debug vreport show" to address remaining aggregate or volume issues.

cluster_B::>
....

. If the configuration uses FC-to-SAS bridges or FC back-end switches, disconnect and remove them.
+
[role="tabbed-block"]
====
.Remove FC-to-SAS bridges
--
.. Identify the bridges:
+
`system bridge show`

.. Remove the bridges:
+
`system bridge remove -name <bridge_name>`

.. Confirm the bridges are removed:
+
`system bridge show`

The following example shows that the bridges are removed:

.Example
[%collapsible]
=====
----
cluster1::> system bridge remove -name ATTO_10.226.197.16
cluster1::> system bridge show
                                                                Is        Monitor
    Bridge     Symbolic Name Vendor  Model     Bridge WWN       Monitored Status
    ---------- ------------- ------- --------- ---------------- --------- -------
    ATTO_FibreBridge6500N_1
               Bridge Number 16
                             Atto    FibreBridge 6500N
                                               2000001086603824 false     -
    ATTO_FibreBridge6500N_2
               Not Set       Atto    FibreBridge 6500N
                                               20000010866037e8 false     -
    ATTO_FibreBridge6500N_3
               Not Set       Atto    FibreBridge 6500N
                                               2000001086609e0e false     -
    ATTO_FibreBridge6500N_4
               Not Set       Atto    FibreBridge 6500N
                                               2000001086609c06 false     -
    4 entries were displayed.
----
=====
--
.Remove FC switches
--
.. Identify the switches:
+
`system switch fibre-channel show`

.. Remove the switches:
+
`system switch fibre-channel remove -switch-name <switch_name>`

.. Confirm the switches are removed:
+
`system switch fibre-channel show`

.Example
[%collapsible]
=====
----
cluster1::> system switch fibre-channel show
                Symbolic                                     Is        Monitor
    Switch      Name     Vendor  Model      Switch WWN       Monitored Status
    ----------- -------- ------- ---------- ---------------- --------- -------
    Cisco_10.226.197.34
                mcc-cisco-8Gb-fab-4
                         Cisco   DS-C9148-16P-K9
                                            2000547fee78f088 true      ok
    mcc-cisco-8Gb-fab-1
                mcc-cisco-8Gb-fab-1
                         Cisco   -          -                false     -
    mcc-cisco-8Gb-fab-2
                mcc-cisco-8Gb-fab-2
                         Cisco   -          -                false     -
    mcc-cisco-8Gb-fab-3
                mcc-cisco-8Gb-fab-3
                         Cisco   -          -                false     -
    4 entries were displayed.
cluster1::> system switch fibre-channel remove -switch-name Cisco_10.226.197.34
cluster1::> system switch fibre-channel show
                Symbolic                                     Is        Monitor
    Switch      Name     Vendor  Model      Switch WWN       Monitored Status
    ----------- -------- ------- ---------- ---------------- --------- -------
    mcc-cisco-8Gb-fab-4
                mcc-cisco-8Gb-fab-4
                         Cisco
                                 -          -                false     -
    mcc-cisco-8Gb-fab-1
                mcc-cisco-8Gb-fab-1
                         Cisco   -          -                false     -
    mcc-cisco-8Gb-fab-2
                mcc-cisco-8Gb-fab-2
                         Cisco   -          -                false     -
    mcc-cisco-8Gb-fab-3
                mcc-cisco-8Gb-fab-3
                         Cisco   -          -                false     -
    4 entries were displayed
cluster1::>
----
=====
--
====

. Power down the MetroCluster FC controller modules and storage shelves.
. Disconnect and remove the MetroCluster FC controller modules and storage shelves.

// 2024 Nov 19, ONTAPDOC-2548
// Issue 203, 2022-09-09
// 2023 APR 21, BURT 1546321
